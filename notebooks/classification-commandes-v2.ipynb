{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181720d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shadow\\anaconda3\\envs\\hermes\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extraction d'entit√©s (slots) depuis les commandes utilisateur\n",
    "Plusieurs approches : Regex, SpaCy NER, et mod√®le seq2seq\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Any, List, Optional\n",
    "import torch\n",
    "from transformers import (\n",
    "    CamembertTokenizerFast,  # Chang√© de CamembertTokenizer √† CamembertTokenizerFast\n",
    "    CamembertForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Todo si erreur (windows ) : pip install \"numpy<2\"\n",
    "# pip install transformers[torch] sentencepiece protobuf\n",
    "# pip install --upgrade transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d49aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# APPROCHE 1: R√®gles et Regex (Simple et efficace pour cas simples)\n",
    "# ============================================================================\n",
    "\n",
    "class RuleBasedSlotExtractor:\n",
    "    \"\"\"Extracteur bas√© sur des r√®gles pour chaque intention\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'export_notation': [\n",
    "                (r'notation[s]?\\s+(?:num√©ro\\s+)?(\\d+)', 'notation_id'),\n",
    "                (r'n¬∞\\s*(\\d+)', 'notation_id'),\n",
    "                (r'la\\s+(\\d+)', 'notation_id'),\n",
    "                (r'(?:^|\\s)(\\d+)(?:$|\\s)', 'notation_id'),  # nombre seul\n",
    "            ],\n",
    "            'creer_notation': [\n",
    "                (r'notation\\s+\"([^\"]+)\"', 'notation_nom'),\n",
    "                (r'notation\\s+(.+?)(?:\\s+pour|\\s+sur|$)', 'notation_nom'),\n",
    "            ],\n",
    "            'creer_essai': [\n",
    "                (r'essai\\s+\"([^\"]+)\"', 'essai_nom'),\n",
    "                (r'essai\\s+(.+?)(?:\\s+pour|\\s+sur|$)', 'essai_nom'),\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def extract(self, text: str, intent: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extrait les slots pour une intention donn√©e\"\"\"\n",
    "        slots = {}\n",
    "        \n",
    "        if intent not in self.patterns:\n",
    "            return slots\n",
    "        \n",
    "        for pattern, slot_name in self.patterns[intent]:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                value = match.group(1)\n",
    "                slots[slot_name] = value.strip()\n",
    "                break  # Prend le premier match\n",
    "        \n",
    "        return slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a0b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# APPROCHE 2: NER avec CamemBERT fine-tun√© (Plus flexible)\n",
    "# ============================================================================\n",
    "\n",
    "class NERSlotExtractor:\n",
    "    \"\"\"\n",
    "    Extraction de slots avec un mod√®le NER (Named Entity Recognition)\n",
    "    Utilise BIO tagging: B-notation_id, I-notation_id, O\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Labels BIO pour chaque type d'entit√©\n",
    "        self.label_list = [\n",
    "            'O',  # Outside (pas une entit√©)\n",
    "            'B-notation_id',  # Begin notation_id\n",
    "            'B-essai_id',\n",
    "            'B-nom',  # Nom g√©n√©rique\n",
    "            'I-nom',  # Inside nom (continuation)\n",
    "        ]\n",
    "        \n",
    "        self.label2id = {label: i for i, label in enumerate(self.label_list)}\n",
    "        self.id2label = {i: label for i, label in enumerate(self.label_list)}\n",
    "    \n",
    "    def create_training_data(self):\n",
    "        \"\"\"Cr√©e des donn√©es d'entra√Ænement annot√©es au format BIO\"\"\"\n",
    "        # Format: (texte, [(d√©but, fin, label)])\n",
    "        examples = [\n",
    "            (\"export de la notation 123\", [(22, 25, 'notation_id')]),\n",
    "            (\"exporter notation 456\", [(18, 21, 'notation_id')]),\n",
    "            (\"je veux exporter la notation num√©ro 789\", [(37, 40, 'notation_id')]),\n",
    "            (\"export notation 12\", [(16, 18, 'notation_id')]),\n",
    "            (\"cr√©er une notation\", []),\n",
    "            (\"cr√©er un essai test\", [(17, 21, 'nom')]),\n",
    "            (\"nouvelle notation pour analyse\", [(22, 30, 'nom')]),\n",
    "        ]\n",
    "        \n",
    "        tokenized_examples = []\n",
    "        tokenizer = CamembertTokenizerFast.from_pretrained('camembert-base')\n",
    "        \n",
    "        for text, entities in examples:\n",
    "            # Tokenisation\n",
    "            encoding = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "            tokens = encoding.tokens()\n",
    "            offsets = encoding['offset_mapping']\n",
    "            \n",
    "            # Cr√©ation des labels BIO\n",
    "            labels = ['O'] * len(tokens)\n",
    "            \n",
    "            for start, end, entity_type in entities:\n",
    "                # Trouve les tokens correspondants\n",
    "                for idx, (token_start, token_end) in enumerate(offsets):\n",
    "                    if token_start >= start and token_end <= end:\n",
    "                        if token_start == start:\n",
    "                            labels[idx] = f'B-{entity_type}'\n",
    "                        else:\n",
    "                            labels[idx] = f'I-{entity_type}'\n",
    "            \n",
    "            tokenized_examples.append({\n",
    "                'text': text,\n",
    "                'tokens': tokens,\n",
    "                'labels': labels,\n",
    "                'input_ids': encoding['input_ids'],\n",
    "                'attention_mask': encoding['attention_mask']\n",
    "            })\n",
    "        \n",
    "        return tokenized_examples\n",
    "    \n",
    "    def extract_from_bio(self, tokens: List[str], labels: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Convertit les labels BIO en dictionnaire de slots\"\"\"\n",
    "        slots = {}\n",
    "        current_entity = None\n",
    "        current_value = []\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label.startswith('B-'):\n",
    "                # Sauvegarde l'entit√© pr√©c√©dente si existante\n",
    "                if current_entity:\n",
    "                    slots[current_entity] = ' '.join(current_value)\n",
    "                \n",
    "                # Commence une nouvelle entit√©\n",
    "                current_entity = label[2:]  # Enl√®ve \"B-\"\n",
    "                current_value = [token]\n",
    "            \n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                # Continue l'entit√© courante\n",
    "                current_value.append(token)\n",
    "            \n",
    "            else:  # O\n",
    "                # Sauvegarde l'entit√© pr√©c√©dente si existante\n",
    "                if current_entity:\n",
    "                    slots[current_entity] = ' '.join(current_value)\n",
    "                    current_entity = None\n",
    "                    current_value = []\n",
    "        \n",
    "        # Sauvegarde la derni√®re entit√©\n",
    "        if current_entity:\n",
    "            slots[current_entity] = ' '.join(current_value)\n",
    "        \n",
    "        return slots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c3d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# ============================================================================\n",
    "# APPROCHE 3: Template avec LLM (Plus puissant mais plus co√ªteux)\n",
    "# ============================================================================\n",
    "\n",
    "class MistralSlotExtractor:\n",
    "    \"\"\"\n",
    "    Extracteur de slots utilisant Mistral-7B-Instruct en local\n",
    "    Optimis√© pour l'extraction d'informations structur√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        local_model_path: str = \"../models/mistral-7b-instruct\",  # Nouveau param√®tre\n",
    "        device: str = \"auto\",\n",
    "        load_in_8bit: bool = False,\n",
    "        load_in_4bit: bool = True\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialise le mod√®le Mistral\n",
    "        \n",
    "        Args:\n",
    "            model_name: Nom du mod√®le sur HuggingFace\n",
    "            device: Device √† utiliser ('cuda', 'cpu', ou 'auto')\n",
    "            load_in_8bit: Charger en quantification 8-bit (n√©cessite bitsandbytes)\n",
    "            load_in_4bit: Charger en quantification 4-bit (recommand√©, √©conomise de la VRAM)\n",
    "        \"\"\"\n",
    "           \n",
    "        # V√©rifier si le mod√®le existe d√©j√† en local\n",
    "        if os.path.exists(local_model_path):\n",
    "            print(f\"üîÑ Chargement du mod√®le depuis {local_model_path}...\")\n",
    "            model_name = local_model_path\n",
    "        else:\n",
    "            print(f\"üîÑ Chargement du mod√®le {model_name} depuis HuggingFace...\")\n",
    "        \n",
    "        print(torch.cuda.is_available())\n",
    "        self.device = device if device != \"auto\" else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Configuration de quantification pour √©conomiser la m√©moire\n",
    "        kwargs = {}\n",
    "        if load_in_4bit and self.device == \"cuda\":\n",
    "            kwargs[\"load_in_4bit\"] = True\n",
    "            kwargs[\"device_map\"] = \"auto\"\n",
    "            print(\"   Using 4-bit quantization\")\n",
    "        elif load_in_8bit and self.device == \"cuda\":\n",
    "            kwargs[\"load_in_8bit\"] = True\n",
    "            kwargs[\"device_map\"] = \"auto\"\n",
    "            print(\"   Using 8-bit quantization\")\n",
    "        else:\n",
    "            kwargs[\"torch_dtype\"] = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        \n",
    "        # Chargement\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "        \n",
    "        if not load_in_4bit and not load_in_8bit:\n",
    "            self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Sauvegarder en local si ce n'est pas d√©j√† fait\n",
    "        if not os.path.exists(local_model_path):\n",
    "            print(f\"üíæ Sauvegarde du mod√®le dans {local_model_path}...\")\n",
    "            os.makedirs(local_model_path, exist_ok=True)\n",
    "            self.tokenizer.save_pretrained(local_model_path)\n",
    "            self.model.save_pretrained(local_model_path)\n",
    "            print(\"‚úì Mod√®le sauvegard√©\")\n",
    "        \n",
    "        print(f\"‚úì Mod√®le charg√© sur {self.device}\")\n",
    "        \n",
    "        # D√©finition des templates pour chaque intention\n",
    "        self.templates = {\n",
    "            'export_notation': {\n",
    "                'slots': ['notation_id'],\n",
    "                'description': 'notation_id est le num√©ro/identifiant de la notation √† exporter (nombre uniquement)'\n",
    "            },\n",
    "            'creer_notation': {\n",
    "                'slots': ['nom', 'description'],\n",
    "                'description': 'nom est le nom de la nouvelle notation, description est une description optionnelle'\n",
    "            },\n",
    "            'creer_essai': {\n",
    "                'slots': ['nom', 'type'],\n",
    "                'description': \"nom est le nom du nouvel essai, type est le type d'essai (optionnel)\"\n",
    "            },\n",
    "            'modifier_essai': {\n",
    "                'slots': ['essai_id', 'champ', 'valeur'],\n",
    "                'description': 'essai_id est l\\'identifiant de l\\'essai, champ est le champ √† modifier, valeur est la nouvelle valeur'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def build_prompt(self, text: str, intent: str) -> str:\n",
    "        \"\"\"\n",
    "        Construit le prompt pour Mistral au format Instruct\n",
    "        \"\"\"\n",
    "        template_info = self.templates.get(intent, {})\n",
    "        slots = template_info.get('slots', [])\n",
    "        description = template_info.get('description', '')\n",
    "        \n",
    "        # Construction du JSON de sortie attendu\n",
    "        json_schema = {slot: \"valeur ou null\" for slot in slots}\n",
    "        json_example = json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Prompt optimis√© pour l'extraction\n",
    "        user_message = f\"\"\"Extrait les informations structur√©es de la phrase suivante.\n",
    "\n",
    "Phrase: \"{text}\"\n",
    "Intention: {intent}\n",
    "\n",
    "Informations √† extraire:\n",
    "{description}\n",
    "\n",
    "R√©ponds UNIQUEMENT avec un objet JSON valide, sans texte avant ou apr√®s.\n",
    "Format attendu:\n",
    "{json_example}\n",
    "\n",
    "Si une information n'est pas pr√©sente dans la phrase, utilise null.\"\"\"\n",
    "\n",
    "        return user_message\n",
    "    \n",
    "    def format_mistral_prompt(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Formate le prompt au format Mistral Instruct\n",
    "        \"\"\"\n",
    "        return f\"<s>[INST] {user_message} [/INST]\"\n",
    "    \n",
    "    def generate(self, prompt: str, max_new_tokens: int = 150, temperature: float = 0.1) -> str:\n",
    "        \"\"\"\n",
    "        G√©n√®re une r√©ponse avec le mod√®le Mistral\n",
    "        \n",
    "        Args:\n",
    "            prompt: Le prompt format√©\n",
    "            max_new_tokens: Nombre maximum de tokens √† g√©n√©rer\n",
    "            temperature: Temp√©rature de g√©n√©ration (plus bas = plus d√©terministe)\n",
    "        \"\"\"\n",
    "        # Tokenisation\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # G√©n√©ration\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=temperature > 0,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # D√©codage (enl√®ve le prompt)\n",
    "        generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return generated_text.strip()\n",
    "    \n",
    "    def extract_json_from_response(self, response: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Extrait un objet JSON de la r√©ponse (g√®re les cas o√π le mod√®le ajoute du texte)\n",
    "        \"\"\"\n",
    "        # M√©thode 1: Chercher un bloc JSON avec regex\n",
    "        json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "        matches = re.findall(json_pattern, response, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # M√©thode 2: Essayer de parser directement\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # M√©thode 3: Nettoyer et r√©essayer\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith('```json'):\n",
    "            cleaned = cleaned[7:]\n",
    "        if cleaned.startswith('```'):\n",
    "            cleaned = cleaned[3:]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned.strip())\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    def extract(self, text: str, intent: str, verbose: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extrait les slots depuis le texte pour l'intention donn√©e\n",
    "        \n",
    "        Args:\n",
    "            text: La phrase utilisateur\n",
    "            intent: L'intention d√©tect√©e\n",
    "            verbose: Affiche les d√©tails de g√©n√©ration\n",
    "        \n",
    "        Returns:\n",
    "            Dictionnaire des slots extraits\n",
    "        \"\"\"\n",
    "        # Construction du prompt\n",
    "        user_message = self.build_prompt(text, intent)\n",
    "        prompt = self.format_mistral_prompt(user_message)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüìù Prompt envoy√© au mod√®le:\")\n",
    "            print(f\"{user_message}\\n\")\n",
    "        \n",
    "        # G√©n√©ration\n",
    "        response = self.generate(prompt)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ü§ñ R√©ponse brute du mod√®le:\")\n",
    "            print(f\"{response}\\n\")\n",
    "        \n",
    "        # Extraction du JSON\n",
    "        slots = self.extract_json_from_response(response)\n",
    "        \n",
    "        if slots is None:\n",
    "            if verbose:\n",
    "                print(\"‚ö†Ô∏è  √âchec de l'extraction JSON, retour d'un dict vide\")\n",
    "            return {}\n",
    "        \n",
    "        # Nettoyage des valeurs null\n",
    "        slots = {k: v for k, v in slots.items() if v is not None and v != \"null\"}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úì Slots extraits: {slots}\")\n",
    "        \n",
    "        return slots\n",
    "    \n",
    "    def extract_batch(self, texts_and_intents: list, verbose: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Extrait les slots pour plusieurs phrases en batch\n",
    "        \n",
    "        Args:\n",
    "            texts_and_intents: Liste de tuples (text, intent)\n",
    "            verbose: Affiche les d√©tails\n",
    "        \n",
    "        Returns:\n",
    "            Liste de dictionnaires de slots\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for text, intent in texts_and_intents:\n",
    "            if verbose:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"Traitement: '{text}' (intent: {intent})\")\n",
    "                print('='*70)\n",
    "            \n",
    "            slots = self.extract(text, intent, verbose=verbose)\n",
    "            results.append(slots)\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb0d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# APPROCHE 4: Syst√®me hybride (Recommand√©)\n",
    "# ============================================================================\n",
    "\n",
    "class HybridSlotExtractor:\n",
    "    \"\"\"\n",
    "    Combine plusieurs approches:\n",
    "    1. Essaie d'abord les r√®gles (rapide, pr√©cis pour cas simples)\n",
    "    2. Si insuffisant, utilise NER ou LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_extractor = RuleBasedSlotExtractor()\n",
    "        # self.ner_extractor = NERSlotExtractor()  # Si mod√®le entra√Æn√©\n",
    "        self.llm_extractor = MistralSlotExtractor(device=\"cpu\")\n",
    "    \n",
    "    def extract(self, text: str, intent: str, use_llm: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extrait les slots en utilisant la meilleure approche\n",
    "        \n",
    "        Args:\n",
    "            text: La phrase utilisateur\n",
    "            intent: L'intention d√©tect√©e\n",
    "            use_llm: Si True, utilise le LLM pour les cas complexes\n",
    "        \"\"\"\n",
    "        # Essaie d'abord les r√®gles\n",
    "        slots = self.rule_extractor.extract(text, intent)\n",
    "        \n",
    "        # Si on a trouv√© des slots, c'est bon\n",
    "        if slots:\n",
    "            return {'method': 'rules', 'slots': slots, 'confidence': 'high'}\n",
    "        \n",
    "        # Sinon, utilise le LLM si demand√©\n",
    "        if use_llm:\n",
    "            slots = self.llm_extractor.extract(text, intent)\n",
    "            return {'method': 'llm', 'slots': slots, 'confidence': 'medium'}\n",
    "        \n",
    "        return {'method': 'none', 'slots': {}, 'confidence': 'low'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c9cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "D√âMONSTRATION D'EXTRACTION DE SLOTS\n",
      "======================================================================\n",
      "\n",
      "üîß APPROCHE 1: R√àGLES ET REGEX\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìù 'export de la notation 345'\n",
      "   Intent: export_notation\n",
      "   Slots: {'notation_id': '345'}\n",
      "\n",
      "üìù 'je veux exporter la notation num√©ro 789'\n",
      "   Intent: export_notation\n",
      "   Slots: {'notation_id': '789'}\n",
      "\n",
      "üìù 'exporter n¬∞ 123'\n",
      "   Intent: export_notation\n",
      "   Slots: {'notation_id': '123'}\n",
      "\n",
      "üìù 'cr√©er une notation analyse des risques'\n",
      "   Intent: creer_notation\n",
      "   Slots: {'notation_nom': 'analyse des risques'}\n",
      "\n",
      "üìù 'nouveau essai de r√©sistance'\n",
      "   Intent: creer_essai\n",
      "   Slots: {'essai_nom': 'de r√©sistance'}\n",
      "\n",
      "\n",
      "ü§ñ APPROCHE 2: NER (Named Entity Recognition)\n",
      "----------------------------------------------------------------------\n",
      "N√©cessite un mod√®le entra√Æn√© sur des donn√©es annot√©es BIO\n",
      "Exemple de donn√©es d'entra√Ænement (7 exemples):\n",
      "\n",
      "  Exemple 1:\n",
      "    Texte: export de la notation 123\n",
      "    Tokens: ['<s>', '‚ñÅexport', '‚ñÅde', '‚ñÅla', '‚ñÅnotation', '‚ñÅ123', '</s>']...\n",
      "    Labels: ['O', 'O', 'O', 'O', 'O', 'B-notation_id', 'O']...\n",
      "\n",
      "  Exemple 2:\n",
      "    Texte: exporter notation 456\n",
      "    Tokens: ['<s>', '‚ñÅexport', 'er', '‚ñÅnotation', '‚ñÅ4', '56', '</s>']...\n",
      "    Labels: ['O', 'O', 'O', 'O', 'B-notation_id', 'I-notation_id', 'O']...\n",
      "\n",
      "\n",
      "üß† APPROCHE 3: LLM (Large Language Model)\n",
      "----------------------------------------------------------------------\n",
      "üîÑ Chargement du mod√®le mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:07<00:00, 62.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le charg√© sur cpu\n",
      "\n",
      "üìù 'export de la notation 345'\n",
      "   Intent: export_notation\n",
      "   Slots: {'notation_id': '345'}\n",
      "\n",
      "üìù 'je veux exporter la notation num√©ro 789'\n",
      "   Intent: export_notation\n",
      "   Slots: {'notation_id': '789'}\n",
      "\n",
      "üìù 'exporter n¬∞ 123'\n",
      "   Intent: export_notation\n",
      "   Slots: {'notation_id': '123'}\n",
      "\n",
      "\n",
      "‚ö° APPROCHE 4: SYST√àME HYBRIDE (RECOMMAND√â)\n",
      "----------------------------------------------------------------------\n",
      "üîÑ Chargement du mod√®le mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:03<00:00, 41.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le charg√© sur cpu\n",
      "\n",
      "üìù 'export de la notation 345'\n",
      "   Intent: export_notation\n",
      "   M√©thode: rules\n",
      "   Confiance: high\n",
      "   Slots: {'notation_id': '345'}\n",
      "\n",
      "üìù 'je veux exporter la notation num√©ro 789'\n",
      "   Intent: export_notation\n",
      "   M√©thode: rules\n",
      "   Confiance: high\n",
      "   Slots: {'notation_id': '789'}\n",
      "\n",
      "üìù 'exporter n¬∞ 123'\n",
      "   Intent: export_notation\n",
      "   M√©thode: rules\n",
      "   Confiance: high\n",
      "   Slots: {'notation_id': '123'}\n",
      "\n",
      "üìù 'cr√©er une notation analyse des risques'\n",
      "   Intent: creer_notation\n",
      "   M√©thode: rules\n",
      "   Confiance: high\n",
      "   Slots: {'notation_nom': 'analyse des risques'}\n",
      "\n",
      "üìù 'nouveau essai de r√©sistance'\n",
      "   Intent: creer_essai\n",
      "   M√©thode: rules\n",
      "   Confiance: high\n",
      "   Slots: {'essai_nom': 'de r√©sistance'}\n",
      "\n",
      "\n",
      "üí° RECOMMANDATIONS\n",
      "======================================================================\n",
      "\n",
      "1. R√àGLES + REGEX (Recommand√© pour commencer):\n",
      "   ‚úì Simple √† impl√©menter et maintenir\n",
      "   ‚úì Tr√®s rapide (< 1ms)\n",
      "   ‚úì Pr√©cis pour les patterns connus\n",
      "   ‚úì Pas besoin d'entra√Ænement\n",
      "   ‚úó Limit√© aux patterns pr√©d√©finis\n",
      "   \n",
      "2. NER (Pour cas plus complexes):\n",
      "   ‚úì Plus flexible que les r√®gles\n",
      "   ‚úì Peut g√©rer des variations\n",
      "   ‚úì Bonne pr√©cision avec peu de donn√©es (100-200 exemples)\n",
      "   ‚úó N√©cessite annotation des donn√©es\n",
      "   ‚úó Temps d'entra√Ænement\n",
      "   \n",
      "3. LLM (Pour flexibilit√© maximale):\n",
      "   ‚úì Tr√®s flexible, comprend le contexte\n",
      "   ‚úì Pas besoin d'entra√Ænement\n",
      "   ‚úì G√®re les cas complexes\n",
      "   ‚úó Co√ªt API ou ressources GPU\n",
      "   ‚úó Latence plus √©lev√©e (100-1000ms)\n",
      "   ‚úó Moins pr√©dictible\n",
      "   \n",
      "4. HYBRIDE (Meilleur compromis):\n",
      "   ‚úì Combine avantages de chaque approche\n",
      "   ‚úì R√®gles pour cas simples (rapide)\n",
      "   ‚úì LLM pour cas complexes (flexible)\n",
      "   ‚úì Optimise co√ªt/performance\n",
      "   \n",
      "end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# D√âMONSTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def demo():\n",
    "    \"\"\"D√©monstration des diff√©rentes approches\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"D√âMONSTRATION D'EXTRACTION DE SLOTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"export de la notation 345\", \"export_notation\"),\n",
    "        (\"je veux exporter la notation num√©ro 789\", \"export_notation\"),\n",
    "        (\"exporter n¬∞ 123\", \"export_notation\"),\n",
    "        (\"cr√©er une notation analyse des risques\", \"creer_notation\"),\n",
    "        (\"nouveau essai de r√©sistance\", \"creer_essai\"),\n",
    "    ]\n",
    "    \n",
    "    # Approche 1: R√®gles\n",
    "    print(\"\\nüîß APPROCHE 1: R√àGLES ET REGEX\")\n",
    "    print(\"-\" * 70)\n",
    "    rule_extractor = RuleBasedSlotExtractor()\n",
    "    \n",
    "    for text, intent in test_cases:\n",
    "        slots = rule_extractor.extract(text, intent)\n",
    "        print(f\"\\nüìù '{text}'\")\n",
    "        print(f\"   Intent: {intent}\")\n",
    "        print(f\"   Slots: {slots}\")\n",
    "    \n",
    "    # Approche 2: NER\n",
    "    print(\"\\n\\nü§ñ APPROCHE 2: NER (Named Entity Recognition)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"N√©cessite un mod√®le entra√Æn√© sur des donn√©es annot√©es BIO\")\n",
    "    ner_extractor = NERSlotExtractor()\n",
    "    training_data = ner_extractor.create_training_data()\n",
    "    print(f\"Exemple de donn√©es d'entra√Ænement ({len(training_data)} exemples):\")\n",
    "    for i, example in enumerate(training_data[:2]):\n",
    "        print(f\"\\n  Exemple {i+1}:\")\n",
    "        print(f\"    Texte: {example['text']}\")\n",
    "        print(f\"    Tokens: {example['tokens'][:10]}...\")\n",
    "        print(f\"    Labels: {example['labels'][:10]}...\")\n",
    "    \n",
    "    # Approche 3: LLM\n",
    "    print(\"\\n\\nüß† APPROCHE 3: LLM (Large Language Model)\")\n",
    "    print(\"-\" * 70)\n",
    "    llm_extractor = MistralSlotExtractor()\n",
    "    \n",
    "    for text, intent in test_cases[:3]:\n",
    "        slots = llm_extractor.extract(text, intent)\n",
    "        print(f\"\\nüìù '{text}'\")\n",
    "        print(f\"   Intent: {intent}\")\n",
    "        print(f\"   Slots: {slots}\")\n",
    "    \n",
    "    # Approche 4: Hybride\n",
    "    print(\"\\n\\n‚ö° APPROCHE 4: SYST√àME HYBRIDE (RECOMMAND√â)\")\n",
    "    print(\"-\" * 70)\n",
    "    hybrid_extractor = HybridSlotExtractor()\n",
    "    \n",
    "    for text, intent in test_cases:\n",
    "        result = hybrid_extractor.extract(text, intent)\n",
    "        print(f\"\\nüìù '{text}'\")\n",
    "        print(f\"   Intent: {intent}\")\n",
    "        print(f\"   M√©thode: {result['method']}\")\n",
    "        print(f\"   Confiance: {result['confidence']}\")\n",
    "        print(f\"   Slots: {result['slots']}\")\n",
    "    \n",
    "    # Recommandations\n",
    "    print(\"\\n\\nüí° RECOMMANDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "1. R√àGLES + REGEX (Recommand√© pour commencer):\n",
    "   ‚úì Simple √† impl√©menter et maintenir\n",
    "   ‚úì Tr√®s rapide (< 1ms)\n",
    "   ‚úì Pr√©cis pour les patterns connus\n",
    "   ‚úì Pas besoin d'entra√Ænement\n",
    "   ‚úó Limit√© aux patterns pr√©d√©finis\n",
    "   \n",
    "2. NER (Pour cas plus complexes):\n",
    "   ‚úì Plus flexible que les r√®gles\n",
    "   ‚úì Peut g√©rer des variations\n",
    "   ‚úì Bonne pr√©cision avec peu de donn√©es (100-200 exemples)\n",
    "   ‚úó N√©cessite annotation des donn√©es\n",
    "   ‚úó Temps d'entra√Ænement\n",
    "   \n",
    "3. LLM (Pour flexibilit√© maximale):\n",
    "   ‚úì Tr√®s flexible, comprend le contexte\n",
    "   ‚úì Pas besoin d'entra√Ænement\n",
    "   ‚úì G√®re les cas complexes\n",
    "   ‚úó Co√ªt API ou ressources GPU\n",
    "   ‚úó Latence plus √©lev√©e (100-1000ms)\n",
    "   ‚úó Moins pr√©dictible\n",
    "   \n",
    "4. HYBRIDE (Meilleur compromis):\n",
    "   ‚úì Combine avantages de chaque approche\n",
    "   ‚úì R√®gles pour cas simples (rapide)\n",
    "   ‚úì LLM pour cas complexes (flexible)\n",
    "   ‚úì Optimise co√ªt/performance\n",
    "   \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae9838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Chargement du mod√®le depuis ../models/mistral-7b-instruct...\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 54.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le charg√© sur cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'method': 'rules', 'slots': {'notation_id': '415'}, 'confidence': 'high'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_extractor = HybridSlotExtractor()\n",
    "\n",
    "result = hybrid_extractor.extract(\"Je voudrais exporter les r√©sultats de la notation 415\", \"export_notation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4b62e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'rules', 'slots': {'notation_id': '567'}, 'confidence': 'high'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = hybrid_extractor.extract(\"Je voudrais exporter la notation 567\", \"export_notation\", use_llm=True)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
