{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab86295d",
   "metadata": {},
   "source": [
    "## A propos du modèle de classification\n",
    "\n",
    "En analysant ce code, il s'agit clairement de fine-tuning (ajustement fin).\n",
    "Justification\n",
    "C'est du fine-tuning car :\n",
    "\n",
    "Chargement d'un modèle pré-entraîné complet\n",
    "\n",
    "python   model = CamembertForSequenceClassification.from_pretrained(\n",
    "       'camembert-base',\n",
    "       num_labels=len(label_to_id)\n",
    "   )\n",
    "Le modèle CamemBERT pré-entraîné est chargé avec tous ses poids, et seule la couche de classification finale est adaptée au nombre de classes du problème spécifique.\n",
    "\n",
    "Réentraînement sur une tâche spécifique\n",
    "\n",
    "python   optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "Tous les paramètres du modèle (model.parameters()) sont optimisés avec un faible learning rate (2e-5), typique du fine-tuning.\n",
    "\n",
    "Adaptation à une tâche de classification personnalisée\n",
    "Le modèle généraliste CamemBERT est adapté à une tâche de classification de commandes spécifique avec des classes métier définies par label_to_id.\n",
    "\n",
    "Ce n'est PAS :\n",
    "\n",
    "De la distillation : Il n'y a pas de modèle \"enseignant\" qui transfère ses connaissances vers un modèle \"élève\" plus petit. Un seul modèle est entraîné ici.\n",
    "Du simple transfert : Le transfert learning est le concept général, mais ici on va au-delà du simple transfert : on réentraîne activement les poids du modèle (pas seulement la couche finale), ce qui caractérise spécifiquement le fine-tuning.\n",
    "\n",
    "Conclusion : C'est une application classique de fine-tuning d'un modèle de langage pré-entraîné (CamemBERT) pour une tâche de classification de texte en français.RéessayerLDonc toutes les couches du modèle sont figées, sauf la dernière c'est ça ?Non, c'est l'inverse !\n",
    "Dans ce code, toutes les couches du modèle sont entraînées, y compris les couches de CamemBERT (l'encodeur) ET la couche de classification finale.\n",
    "La preuve dans le code :\n",
    "pythonoptimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "model.parameters() retourne tous les paramètres du modèle, donc :\n",
    "\n",
    "Les 12 couches du transformer CamemBERT\n",
    "La couche de classification finale\n",
    "\n",
    "Tous ces paramètres sont mis à jour pendant l'entraînement.\n",
    "Si on voulait figer les couches de base :\n",
    "Pour ne réentraîner que la couche de classification (ce qu'on appelle \"feature extraction\"), il faudrait faire :\n",
    "python# Figer toutes les couches de CamemBERT\n",
    "for param in model.camembert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Seule la couche de classification sera entraînée\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=2e-5)\n",
    "Dans ton code actuel :\n",
    "C'est du fine-tuning complet :\n",
    "\n",
    "✅ Toutes les couches sont entraînées\n",
    "✅ Le learning rate faible (2e-5) permet d'ajuster finement les poids pré-entraînés sans les \"casser\"\n",
    "✅ C'est l'approche la plus courante et généralement la plus performante quand on a suffisamment de données\n",
    "\n",
    "C'est un choix stratégique important en NLP. Voici pourquoi on ne fige généralement pas les couches :\n",
    "1. Adaptation sémantique au domaine\n",
    "Même si CamemBERT connaît le français général, ton vocabulaire métier est spécifique :\n",
    "\n",
    "Les mots ont des sens différents dans ton contexte\n",
    "Les expressions et tournures sont propres à ton domaine\n",
    "Exemple : \"ouvrir un ticket\" a un sens différent de \"ouvrir une porte\"\n",
    "\n",
    "En figeant les couches, le modèle ne pourrait pas apprendre ces nuances sémantiques.\n",
    "2. La couche finale seule est limitée\n",
    "Si tu ne réentraînes que la dernière couche :\n",
    "\n",
    "Tu te contentes de combiner linéairement les représentations existantes\n",
    "C'est comme essayer de résoudre un nouveau problème avec un vocabulaire inadapté\n",
    "Les représentations intermédiaires restent \"génériques\"\n",
    "\n",
    "Avec le fine-tuning complet :\n",
    "\n",
    "Les couches intermédiaires s'adaptent pour extraire les features pertinentes pour ta tâche\n",
    "Le modèle apprend à \"faire attention\" aux bons éléments du texte\n",
    "\n",
    "3. Quand figer les couches alors ?\n",
    "On fige les couches uniquement si :\n",
    "\n",
    "❌ Très peu de données (< 1000 exemples) → risque d'overfitting\n",
    "❌ Tâche très proche du pré-entraînement → les représentations sont déjà bonnes\n",
    "❌ Ressources limitées → entraînement plus rapide\n",
    "\n",
    "4. Dans ton cas\n",
    "Avec train_test_split, tu sembles avoir un dataset raisonnable, donc :\n",
    "\n",
    "✅ Le fine-tuning complet est le meilleur choix\n",
    "✅ Le learning rate faible (2e-5) évite de \"casser\" les bonnes représentations pré-entraînées\n",
    "✅ Tu obtiens un modèle vraiment adapté à ton domaine\n",
    "\n",
    "En résumé : Changer juste la dernière couche serait comme utiliser un dictionnaire français généraliste pour comprendre du jargon technique - ça marche, mais c'est loin d'être optimal !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
